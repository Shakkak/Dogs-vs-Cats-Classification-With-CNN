{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from shutil import copyfile\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# source https://colab.research.google.com/github/lmoroney/mlday-tokyo/blob/master/Lab6-Cats-v-Dogs.ipynb#scrollTo=7v55rWlQehzL\n",
    "\n",
    "# Note: This is a very large dataset and will take time to download\n",
    "\n",
    "!wget --no-check-certificate \\\n",
    "    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n",
    "    -O \"/tmp/cats-and-dogs.zip\"\n",
    "\n",
    "local_zip = '/tmp/cats-and-dogs.zip'\n",
    "zip_ref   = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('/tmp/cats-v-dogs')\n",
    "    os.mkdir('/tmp/cats-v-dogs/training')\n",
    "    os.mkdir('/tmp/cats-v-dogs/testing')\n",
    "    os.mkdir('/tmp/cats-v-dogs/training/cats')\n",
    "    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n",
    "    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n",
    "    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "    files = []\n",
    "    for filename in os.listdir(SOURCE):\n",
    "        file = SOURCE + filename\n",
    "        if os.path.getsize(file) > 0:\n",
    "            files.append(filename)\n",
    "        else:\n",
    "            print(filename + \" is zero length, so ignoring.\")\n",
    "\n",
    "    training_length = int(len(files) * SPLIT_SIZE)\n",
    "    testing_length = int(len(files) - training_length)\n",
    "    shuffled_set = random.sample(files, len(files))\n",
    "    training_set = shuffled_set[0:training_length]\n",
    "    testing_set = shuffled_set[-testing_length:]\n",
    "\n",
    "    for filename in training_set:\n",
    "        this_file = SOURCE + filename\n",
    "        destination = TRAINING + filename\n",
    "        copyfile(this_file, destination)\n",
    "\n",
    "    for filename in testing_set:\n",
    "        this_file = SOURCE + filename\n",
    "        destination = TESTING + filename\n",
    "        copyfile(this_file, destination)\n",
    "\n",
    "\n",
    "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
    "TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n",
    "TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n",
    "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
    "TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n",
    "TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n",
    "\n",
    "split_size = .85\n",
    "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
    "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traininglen = len(os.listdir('/tmp/cats-v-dogs/training/cats/')) + len(os.listdir('/tmp/cats-v-dogs/training/dogs/'))\n",
    "print(Traininglen)\n",
    "\n",
    "# Total of images are 25000, so 85% should be 21250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 - 1 conv layer\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "# Conv Layer 1\n",
    "model1.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "model1.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# flatten\n",
    "model1.add(Flatten())\n",
    "\n",
    "# Fully connected Lyaer\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 - 2 conv layers\n",
    "model2 = Sequential()\n",
    "\n",
    "# Conv Layer 1\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv Layer 2\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(Flatten())\n",
    "\n",
    "# Fully connected Lyaers\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 - 5 conv layers - No Batchnormalization and No dropout\n",
    "model3_NBND = Sequential()\n",
    "\n",
    "# Conv Layer 1\n",
    "model3_NBND.add(Conv2D(64, (3, 3), activation='relu', input_shape=(128,128,3)))\n",
    "model3_NBND.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv Layer 2\n",
    "model3_NBND.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model3_NBND.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv Layer 3\n",
    "model3_NBND.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model3_NBND.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv Layer 4\n",
    "model3_NBND.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model3_NBND.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv Layer 5\n",
    "model3_NBND.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model3_NBND.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model3_NBND.add(Flatten())\n",
    "\n",
    "\n",
    "# Fully connected Lyaers\n",
    "model3_NBND.add(Dense(units=1024, activation='relu'))\n",
    "\n",
    "model3_NBND.add(Dense(units=256, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model3_NBND.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# compile\n",
    "model3_NBND.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agian Model 3 - 5 conv layers - but with Batchnormalization and dropout\n",
    "\n",
    "model3_WBD = Sequential()\n",
    "\n",
    "# Conv Layer 1\n",
    "model3_WBD.add(Conv2D(64, (3, 3), input_shape=(128,128,3)))\n",
    "model3_WBD.add(BatchNormalization(axis=-1))\n",
    "model3_WBD.add(Activation('relu'))\n",
    "model3_WBD.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Conv Layer 2\n",
    "model3_WBD.add(Conv2D(128, (3, 3)))\n",
    "model3_WBD.add(BatchNormalization(axis=-1))\n",
    "model3_WBD.add(Activation('relu'))\n",
    "model3_WBD.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv Layer 3\n",
    "model3_WBD.add(Conv2D(256, (3, 3)))\n",
    "model3_WBD.add(BatchNormalization(axis=-1))\n",
    "model3_WBD.add(Activation('relu'))\n",
    "model3_WBD.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv Layer 4\n",
    "model3_WBD.add(Conv2D(256, (3, 3)))\n",
    "model3_WBD.add(BatchNormalization(axis=-1))\n",
    "model3_WBD.add(Activation('relu'))\n",
    "model3_WBD.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Conv Layer 5\n",
    "model3_WBD.add(Conv2D(512, (3, 3)))\n",
    "model3_WBD.add(BatchNormalization(axis=-1))\n",
    "model3_WBD.add(Activation('relu'))\n",
    "model3_WBD.add(MaxPooling2D(2, 2))\n",
    "\n",
    "# flatten\n",
    "model3_WBD.add(Flatten())\n",
    "\n",
    "# full connection w/ dropout  \n",
    "model3_WBD.add(Dense(units=1024))\n",
    "model3_WBD.add(Activation('relu'))\n",
    "model3_WBD.add(Dropout(0.5))\n",
    "\n",
    "model3_WBD.add(Dense(units=256))\n",
    "model3_WBD.add(Activation('relu')) \n",
    "model3_WBD.add(Dropout(0.5))\n",
    "\n",
    "# output layer\n",
    "model3_WBD.add(Dense(units=1))\n",
    "model3_WBD.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelList = [model1, model2, model3_NBND, model3_NBND]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust learning rate for plateaus\n",
    "# we will start with a higher learning rate and /\n",
    "# set it to reduce by half if our loss does not improve for 3 epochs, /\n",
    "#  with a minimum learning rate of 0.00001.\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, \n",
    "                                            verbose=2, factor=0.5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early-stopping conditions for the training of our model: \\\n",
    "# if the loss does not improve by at least 1e-10 for 25 epochs, then training will stop\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=1e-10, \n",
    "                               patience=25, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image processing, data augmentation\n",
    "\n",
    "Batch = 256\n",
    "\n",
    "TRAINING_DIR = \"/tmp/cats-v-dogs/training/\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rotation_range=15,\n",
    "                                rescale=1./255,\n",
    "                                shear_range=0.1,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                width_shift_range=0.1,\n",
    "                                height_shift_range=0.1\n",
    "                                )\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
    "                                                    batch_size=Batch,\n",
    "                                                    class_mode='binary',\n",
    "                                                    target_size=(128, 128))\n",
    "\n",
    "VALIDATION_DIR = \"/tmp/cats-v-dogs/testing/\"\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rotation_range=15,\n",
    "                                rescale=1./255,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                width_shift_range=0.1,\n",
    "                                height_shift_range=0.1)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
    "                                                              batch_size=Batch,\n",
    "                                                              class_mode='binary',\n",
    "                                                              target_size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "for model in models:\n",
    "    if count == 1:\n",
    "        modelname = \"model1.h5\"\n",
    "    if count == 2:\n",
    "        modelname = \"model2.h5\"\n",
    "    if count == 3:\n",
    "        modelname = \"model3_NBND.h5\"\n",
    "    if count == 4:\n",
    "        modelname = \"model3WD.h5\"\n",
    "\n",
    "    # save Model While Training\n",
    "    best_model = ModelCheckpoint(modelname, monitor='val_acc', verbose=0, \n",
    "                             save_best_only=True, mode='max')\n",
    "    CSV = CSVLogger(modelname, separator=\",\", append=False)\n",
    "\n",
    "    time_initial = time.time() # how much time took\n",
    "    history = model.fit(train_generator, epochs=100, steps_per_epoch= Traininglen/Batch,\n",
    "                    validation_data=validation_generator, validation_steps=6,\n",
    "                    callbacks=[early_stopping, learning_rate_reduction, best_model, CSV])\n",
    "                    \n",
    "    time_elapsed = time.time() - time_initial"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1412564b2367d30a585c4841731c8feaab4a2be725951c84414fc88582b1b6e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('DeepLearning': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
